Configuration tuning is often a key element in achieving outstanding performance with parameterized algorithmic solutions. However, it often requires extensive manual effort to satisfactorily configure an algorithm for a specific task instance, thus preventing these algorithms to be deployed at scale. More importantly, without a principled method to configure these deployment settings, it will be difficult to reproduce the obtained results under other experimental conditions. To address these problems, this thesis focuses on developing novel automated algorithm design (AAD) frameworks capable of configuring algorithms for specific use cases in a data-driven manner. Particularly, we cast these AAD problems as optimization tasks that aim to maximize some performance metric with respect to the configurations of the solution model. We ground our investigation in three specific classes of AAD problems, including kernel selection for Bayesian inference, architecture search for deep neural network and minimizer construction for sequence sketching. In all of these problems, the variables to be optimized often have underlying discrete structures such as trees, graphs or permutations. Our contribution is a suite of reformulation techniques that result in efficient and accurate tuning methods for these configuration domains. Finally, we demonstrate the performance of our methods on practical scenarios and show that they have significantly outperformed state-of-the-art benchmarks.